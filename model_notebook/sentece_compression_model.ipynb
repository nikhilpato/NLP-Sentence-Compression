{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"sentece_compression_model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Xs87gJhT6vJu","colab_type":"code","colab":{}},"source":["# RUN this notebook on Google Colab\n","# Use GPU"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"U6g8RyCIDaFJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8aae4a7b-3877-42fc-a327-787e25b0b8e1","executionInfo":{"status":"ok","timestamp":1576086844175,"user_tz":480,"elapsed":2047,"user":{"displayName":"Roberto Dominguez","photoUrl":"","userId":"05900225780014649715"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8o3ajp017EWM","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","try:\n","  # %tensorflow_version only exists in Colab.\n","  %tensorflow_version 2.x\n","except Exception:\n","  pass\n","\n","import re\n","import os\n","import io\n","import time\n","import json\n","import unicodedata\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import preprocessing\n","from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, concatenate"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-oR_AwXDtRB","colab_type":"code","colab":{}},"source":["%cd # Folder"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K2QT3vcu-4pY","colab_type":"code","colab":{}},"source":["# Config\n","NUM_EXAMPLES = 175000\n","BATCH_SIZE = 64\n","STEPS_PER_EPOCH = NUM_EXAMPLES//BATCH_SIZE\n","WORD_EMB_DIM = 128\n","POS_EMB_DIM = 128\n","DEP_EMB_DIM = 128\n","VOCAB_SIZE = 50002\n","WORD_DICT_SIZE = 50002\n","POS_DICT_SIZE = 53\n","DEP_DICT_SIZE = 54\n","NUM_UNITS = 100\n","NUM_LAYERS = 1\n","LEARNING_RATE = .001\n","DECAY_RATE = .99"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ocGC4_4Y7J1I","colab_type":"code","colab":{}},"source":["def read_data(file_directory, file_name):\n","  \"\"\" Read in the vectorized sentences from the \n","      given file.\n","\n","  Args:\n","    file_directory: The directory with the files with the vectorizes sentences.\n","    file_name: The file that contains the vectroized sentence.\n","\n","  Returns:\n","    data: A list of vectorized sentences.\n","  \n","  \"\"\"\n","  data = []\n","  file_path_name = file_directory + '/' + file_name\n","  with open(file_path_name) as doc:\n","      for line in doc:\n","          data.append([int(x) for x in line.split()])\n","  return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4vX-65cj8HFe","colab_type":"code","colab":{}},"source":["def read_all_data(file_dir, file_base, words, pos, dep, target, num_files):\n","  \"\"\" Read in all the vectorized sentences. Each file\n","      (e.g. train-word_vector1.txt) contains 20,000 vectorized sentences.\n","      This function reads in all the data from all the files.\n","  \n","  Args:\n","    file_dir: The directory with the files with the vectorizes sentences.\n","    file_base: Either 'train', 'val', or 'test'\n","    words: Empty array to place the vectorized words.\n","    pos: Empty array to place the vectorized parts of speech.\n","    dep: Empty array to place the dependency relations.\n","    target: Empty array to place the labels.\n","    num_files: The number of files to read in\n","\n","  Returns:\n","    None\n","    \n","  \"\"\"\n","  for file_id in range(1, num_files+1):\n","    file_tail = '-word_vector' + str(file_id) + '.txt'\n","    words += read_data(file_dir, file_base + file_tail)\n","    file_tail = '-label_vector' + str(file_id) + '.txt'\n","    target += read_data(file_dir, file_base + file_tail)\n","    file_tail = '-pos_vector' + str(file_id) + '.txt'\n","    pos += read_data(file_dir, file_base + file_tail)\n","    file_tail = '-dep_vector' + str(file_id) + '.txt'\n","    dep += read_data(file_dir, file_base + file_tail)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yK6dB_4L9V9o","colab_type":"code","colab":{}},"source":["def read_file(file_name, read_json=False):\n","  \"\"\" Read in the dictionaries or vocabulary.\n","\n","  Args:\n","    file_name: The name of the file that holds the dictionary.\n","    read_json: Whether the file is a json file or a text file.\n","\n","  Returns:\n","    data: The dictionary/vocabulary contained in the file.\n","\n","  \"\"\"\n","  data = []\n","  # currently in 'assets' directory\n","  # change this to the directory containing these dictionaires.\n","  with open('assets/' + file_name) as document:\n","    if read_json:\n","      data = json.load(document)\n","    else:\n","      data += ([str(x) for x in document.read().split()])\n","  return data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VWsPYdmu94I6","colab_type":"code","colab":{}},"source":["# Get vocabulary and dictionaries\n","word_vocab = read_file('word_vocab.txt')\n","word2id = read_file('word_dict.json', read_json=True)\n","pos2id = read_file('pos_dict.json', read_json=True)\n","dep2id = read_file('dep_dict.json', read_json=True)\n","\n","# Compute id to word dictionary\n","id2word = {v: k for k, v in word2id.items()}\n","# Add other values\n","id2word.update({0:'<delete>', 50001:'<unk>', 50002:'<unk>'})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BCirsjzc-bzR","colab_type":"code","colab":{}},"source":["# Create training set\n","train_words_seq = []\n","train_pos_seq = []\n","train_dep_seq = []\n","train_target_seq = []\n","read_all_data('train', 'train', train_words_seq, train_pos_seq, \n","              train_dep_seq, train_target_seq, 9)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bPP313lmGFPV","colab":{}},"source":["# Remove very long sentences\n","train_words_seq = train_words_seq[:-5000]\n","train_pos_seq = train_pos_seq[:-5000]\n","train_dep_seq = train_dep_seq[:-5000]\n","train_target_seq = train_target_seq[:-5000]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QGfY0Cm0-gXE","colab_type":"code","colab":{}},"source":["# Create validation set\n","val_words_seq = []\n","val_pos_seq = []\n","val_dep_seq = []\n","val_target_seq = []\n","read_all_data('validation', 'val', val_words_seq, val_pos_seq, \n","              val_dep_seq, val_target_seq, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oipvG2wuBNve","colab":{}},"source":["# Create test set\n","test_words_seq = []\n","test_pos_seq = []\n","test_dep_seq = []\n","test_target_seq = []\n","read_all_data('test', 'test', test_words_seq, test_pos_seq, \n","              test_dep_seq, test_target_seq, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jUSSje-t-hbU","colab_type":"code","colab":{}},"source":["# pad all sequences\n","train_word_tensor = preprocessing.sequence.pad_sequences(train_words_seq, padding='post')\n","train_pos_tensor = preprocessing.sequence.pad_sequences(train_pos_seq, padding='post')\n","train_dep_tensor = preprocessing.sequence.pad_sequences(train_dep_seq, padding='post')\n","train_target_tensor = preprocessing.sequence.pad_sequences(train_target_seq, padding='post')\n","val_word_tensor = preprocessing.sequence.pad_sequences(val_words_seq, padding='post')\n","val_pos_tensor = preprocessing.sequence.pad_sequences(val_pos_seq, padding='post')\n","val_dep_tensor = preprocessing.sequence.pad_sequences(val_dep_seq, padding='post')\n","val_target_tensor = preprocessing.sequence.pad_sequences(val_target_seq, padding='post')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yb48bwfA-wlp","colab_type":"code","colab":{}},"source":["# Verify that dimensions are all the same\n","def verify_dim(word_seq, pos_seq, dep_seq, target_seq):\n","  for word_sent, pos_sent, dep_sent, target_sent in zip(word_seq, pos_seq, dep_seq, target_seq):\n","    if len(word_sent) == len(pos_sent) == len(dep_sent) == len(target_sent):\n","      continue\n","    else:\n","      print('FAIL')\n","      break"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yw4PRq4o-zUi","colab_type":"code","colab":{}},"source":["verify_dim(train_word_tensor, train_pos_tensor, train_dep_tensor, train_target_tensor)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WxmG5Xrp-04Z","colab_type":"code","colab":{}},"source":["verify_dim(val_word_tensor, val_pos_tensor, val_dep_tensor, val_target_tensor)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6fAFYdGa-7CE","colab_type":"code","colab":{}},"source":["# Create a tf.data Dataset\n","dataset = tf.data.Dataset.from_tensor_slices((train_word_tensor, \n","                                              train_pos_tensor, \n","                                              train_dep_tensor, \n","                                              train_target_tensor)\n","                                            ).shuffle(NUM_EXAMPLES)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eGmUt9z4-90P","colab_type":"code","colab":{}},"source":["# Verify that dimensions are correct\n","w, p, d, t = next(iter(dataset))\n","verify_dim(w, p, d, t)\n","print(w.shape, p.shape, d.shape, t.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V2I1vK46_GcW","colab_type":"code","colab":{}},"source":["# Encoder\n","class Encoder(tf.keras.Model):\n","  \"\"\" Encoder model that compresses a given input sentences.\n","      It recieves three embeddings as input. The word embeddings,\n","      the part-of-speech embeddings, and the dependency relation\n","      embeddings. A bidirectional LSTM is used to extract the \n","      features that are used to predict whether a word is removed\n","      or kept.\n","\n","  Args:\n","    vocab_size: Vocabulary size defined by the training set.\n","    enc_units: The number of units for the LSTM.\n","    batch_size: The batch size.\n","\n","  \"\"\"\n","  def __init__(self, vocab_size, enc_units, batch_sz):\n","    super(Encoder, self).__init__()\n","    self.batch_sz = batch_sz\n","    self.enc_units = enc_units\n","    self.word_embedding = Embedding(WORD_DICT_SIZE, WORD_EMB_DIM, mask_zero=True, embeddings_initializer='glorot_uniform')\n","    self.pos_embedding = Embedding(POS_DICT_SIZE, POS_EMB_DIM, mask_zero=True, embeddings_initializer='glorot_uniform')\n","    self.dep_embedding = Embedding(DEP_DICT_SIZE, DEP_EMB_DIM, mask_zero=True, embeddings_initializer='glorot_uniform')\n","    self.lstm = LSTM(NUM_UNITS, return_sequences=True, return_state=True, dropout=0.5, recurrent_initializer='glorot_uniform')\n","    self.bidirectional = Bidirectional(self.lstm, merge_mode='concat')\n","    self.fc = Dense(1, activation='sigmoid')\n","\n","  def call(self, words, pos, dep):\n","    word_emb = self.word_embedding(words)\n","    pos_emb = self.pos_embedding(pos)\n","    dep_emb = self.dep_embedding(dep)\n","    mask = self.dep_embedding.compute_mask(words)\n","    x = concatenate([word_emb, pos_emb, dep_emb])\n","    x, forward_ouput, forward_state, backward_output, backward_state = self.bidirectional(x, mask=mask)\n","    output = tf.squeeze(self.fc(x), -1)\n","    return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aq5kRV_T_M2-","colab_type":"code","colab":{}},"source":["def loss_function(real, pred):\n","  \"\"\" Loss function.\n","\n","  Args:\n","    real: The ground truth labels.\n","    pred: The predicted probabilities.\n","\n","  Returns:\n","    The average loss.\n","\n","  \"\"\"\n","  labels = tf.math.logical_not(tf.math.equal(real, 0))\n","  new_target = tf.dtypes.cast(labels, tf.float32)\n","  loss_ = loss_object(new_target, pred)\n","  return tf.reduce_mean(loss_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v2Eudy0S_Ls-","colab_type":"code","colab":{}},"source":["optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n","loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction='none')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LYTICpTf_hxC","colab_type":"code","colab":{}},"source":["encoder = Encoder(VOCAB_SIZE, NUM_UNITS, BATCH_SIZE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aF6fnHQE_jlr","colab_type":"code","colab":{}},"source":["checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iq08pMIk_nWu","colab_type":"code","colab":{}},"source":["@tf.function\n","def train_step(words, pos, dep, targ):\n","  \"\"\" Train the model and calculate the loss\n","      for a batch per time step.\n","\n","  Args:\n","    words: The vectorized words for a batch.\n","    pos: The vectorized parts of speech for a batch.\n","    dep: The vectorized dependency relations for a batch.\n","    targ: The labels for a batch.\n","  \n","  Returns:\n","    The loss for a given batch.\n","\n","  \"\"\"\n","  loss = 0\n","\n","  with tf.GradientTape() as tape:\n","    predictions = encoder(words, pos, dep)\n","\n","    # don't include '<bos>'\n","    # calculate loss per batch, per time step\n","    for t in range(1, targ.shape[1]):\n","      loss += loss_function(targ[:, t], predictions[:, t])\n","\n","  batch_loss = (loss / int(targ.shape[1]))\n","  variables = encoder.trainable_variables\n","  gradients = tape.gradient(loss, variables)\n","  optimizer.apply_gradients(zip(gradients, variables))\n","  return batch_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"knYWsmcsAaTC","colab_type":"code","colab":{}},"source":["# Train the Model\n","\n","EPOCHS = 40\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","  total_loss = 0\n","\n","  for (batch, (words, pos, dep, targ)) in enumerate(dataset.take(STEPS_PER_EPOCH)):\n","    batch_loss = train_step(words, pos, dep, targ)\n","    total_loss += batch_loss\n","\n","    if batch % 100 == 0:\n","        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n","                                                     batch,\n","                                                     batch_loss.numpy()))\n","  # saving (checkpoint) the model every 2 epochs\n","  if (epoch + 1) % 2 == 0:\n","    checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n","                                      total_loss / STEPS_PER_EPOCH))\n","  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jzQDv9yIBRhg","colab_type":"code","colab":{}},"source":["# Save Model\n","encoder.save_weights('model/sc_model', save_format='tf')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xdROMos1BYHS","colab_type":"code","colab":{}},"source":["# Load Model\n","enc = Encoder(VOCAB_SIZE, NUM_UNITS, BATCH_SIZE)\n","enc.load_weights('model/sc_model')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aVpu-rZ4G8p8","colab_type":"code","colab":{}},"source":["import nltk\n","from nltk.metrics.distance import edit_distance"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TbjFDq39Bqzy","colab_type":"code","colab":{}},"source":["def evaluate(encoder, words, pos, dep):\n","  \"\"\" Returns the model predictions.\n","\n","  Args:\n","    encoder: The Encoder model.\n","    words: A list of lists of words.\n","    pos: A list of lists of parts of speech.\n","    dep: A list of lists of dependency relations.\n","\n","  Returns:\n","    enc_out: A list of lists of probabilities.\n","\n","  \"\"\"\n","  word_seq = tf.dtypes.cast(tf.convert_to_tensor(words), dtype=tf.dtypes.float32)\n","  pos_seq = tf.dtypes.cast(tf.convert_to_tensor(pos), dtype=tf.dtypes.float32)\n","  dep_seq = tf.dtypes.cast(tf.convert_to_tensor(dep), dtype=tf.dtypes.float32)\n","  enc_out = encoder(word_seq, pos_seq, dep_seq)\n","  return(enc_out)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HRJQKASxB1G-","colab_type":"code","colab":{}},"source":["def get_comp_ratio(res):\n","  \"\"\" Get the compression ratio and percent of reduction.\n","\n","  Args:\n","    res: The sentence generated by the model.\n","\n","  Returns:\n","    Tuple with compression ratio and percent of reduction.\n","\n","  \"\"\"\n","  kept_num = 0\n","  for i in range(len(res[0])):\n","    # use a threshold of .5 to indicate if a word is kept.\n","    if res[0][int(i)]>.5:\n","        kept_num += 1\n","  return (len(res[0])/kept_num, kept_num/(len(res[0]/kept_num)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ESlLuOs1B35k","colab_type":"code","colab":{}},"source":["def get_edit_distance(res, target):\n","  \"\"\" Compute the edit distance between the generated sentence and the \n","      true compressed sentence.\n","\n","  Args:\n","    res: The sentence generated by the model.\n","    target: The true compressed sentence.\n","\n","  Returns:\n","    Edit distance.\n","\n","  \"\"\"\n","  res_str = ''\n","  targ_str = ''\n","  for i in range(len(res[0])):\n","    # convert generated sentence to a bit string\n","    if res[0][int(i)]>.5:\n","      res_str += '1'\n","    else:\n","      res_str += '0'\n","    # convert true sentence to a bit string\n","    if int(target[0][i]) != 0:\n","      targ_str += '1'\n","    else:\n","      targ_str += '0'\n","  return edit_distance(res_str, targ_str)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hixs8imsC8WS","colab_type":"code","colab":{}},"source":["edit_distances = []\n","comp = []\n","comp_perc = []\n","for words, pos, dep, targ in zip(test_words_seq, test_pos_seq, test_dep_seq, test_target_seq):\n","  word_seq = tf.expand_dims(words, 0)\n","  pos_seq = tf.expand_dims(pos, 0)\n","  dep_seq = tf.expand_dims(dep, 0)\n","  target_seq = tf.expand_dims(targ, 0)\n","  res = evaluate(enc, word_seq, pos_seq, dep_seq)\n","  compression = get_comp_ratio(res)\n","  comp.append(compression[0])\n","  comp_perc.append(compression[1])\n","  edit_distances.append(get_edit_distance(res, target_seq))\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"URCwK_k0DNmv","colab_type":"code","colab":{}},"source":["np.mean(comp)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VANj1M8-DPd6","colab_type":"code","colab":{}},"source":["np.mean(comp_perc)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7n6fwdQWDRgh","colab_type":"code","colab":{}},"source":["np.mean(edit_distances)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UV7_xO5vHRjZ","colab_type":"code","colab":{}},"source":["# Create Sample Data\n","sample_word = tf.expand_dims(test_words_seq[0], 0)\n","sample_pos = tf.expand_dims(test_pos_seq[0], 0)\n","sample_dep = tf.expand_dims(test_dep_seq[0], 0)\n","sample_target = tf.expand_dims(test_target_seq[0], 0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BQLgLyBjHYpA","colab_type":"code","colab":{}},"source":["res = evaluate(enc, sample_word, sample_pos, sample_dep)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nx2MRUgMJDWv","colab_type":"code","colab":{}},"source":["# Print original target\n","sample_original = ''\n","for i in sample_word[0]:\n","  if i>0:\n","    sample_original += id2word[int(i)] + ' '\n","print(sample_original)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5w_LJhXvJFZt","colab_type":"code","colab":{}},"source":["# Print sample target\n","sample_truth = ' '\n","for i in sample_target[0]:\n","  if i>0:\n","    sample_truth += id2word[int(i)] + ' '\n","print(sample_truth)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NzVOEgweHeIl","colab_type":"code","colab":{}},"source":["# Print sample result\n","sample_result = ' '\n","for i in range(len(res[0])):\n","  if res[0][int(i)]>.5:\n","    sample_result += id2word[int(sample_word[0][int(i)])] + ' '\n","print(sample_result)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LAimvT-UI8Zq","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}